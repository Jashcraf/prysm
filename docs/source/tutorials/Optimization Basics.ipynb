{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447a848e-6879-4bcb-be5e-e38a6695e073",
   "metadata": {},
   "source": [
    "# Optimization Basics\n",
    "\n",
    "This turorial will introduce the basics of optimization using prysm's `x/optym` experimental module.  We will:\n",
    "\n",
    "1.  Discuss the kind of optimization prysm is able to perform\n",
    "2.  Show how the machinery works using a common toy function\n",
    "\n",
    "At the end of this tutorial, you will be able to use `x/optym` to solve optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4abc9b-dfe4-4fbd-a3e0-c878cdc8d869",
   "metadata": {},
   "source": [
    "## Gradient-Based Optimization\n",
    "\n",
    "All of the optimizers within prysm are gradient-based, meaning they require you be able to compute some _scalar_ loss function $\\mathcal{L}(x)$, and its gradient $\\nabla = \\langle \\partial\\mathcal{L}/\\partial x_1,\\partial\\mathcal{L}/\\partial x_2, \\ldots, \\partial\\mathcal{L}/\\partial x_N \\rangle$.  You may also see the loss function called an objective function, a merit function, or a cost function.  You may also see the parameter vector $x$ as a $\\theta$.  prysm features only gradient-based optimizers, for gradient-free optimizers like Nelder-Meade or Hessian-based optimizers like Newton's method, you will have to look elsewhere for now.\n",
    "\n",
    "Computing $\\nabla$ may seem intractible on its face, but using algorithmic differentiation built into prysm, it is straightforward and most of the things you might want to differentiate with respect to are available batteries included today.  We'll start with a toy problem, the rosenbrock function from `scipy.optimize`, which is very difficult to minimize.  Scipy includes both it and and its derivative, so we'll import those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6646ca9-dc07-4843-b216-7ab98c93ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import rosen, rosen_der"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33b0a8-8b32-4f89-803e-8a5ec2c6adc4",
   "metadata": {},
   "source": [
    "`rosen` is the $\\mathcal{L}(x)$ function itself, and `rosen_der` is the derivative $\\nabla\\mathcal{L}(x)$.  prysm's optimizer implementations expect to be given a function `def cost_grad(x: ndarray) -> float, ndarray`.  This is different to scipy, where you provide the two as separate functions.  They are grouped in prysm because optimizers that use both tend to look at each value at the same time, and most code computes $\\mathcal{L}$ almost as a byproduct of computing $\\nabla$, so this interface is faster.\n",
    "\n",
    "We will make a simple wrapper around scipy's functions to meet the interface requirement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd6f23-abf2-4d8b-a086-f974f691300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_grad(x):\n",
    "    f = rosen(x)\n",
    "    g = rosen_der(x)\n",
    "    return f, g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37016b-9f89-4c8c-921f-dffba1f7734a",
   "metadata": {},
   "source": [
    "We are now ready to optimize.  The complete list of available optimizers is available [in the API reference for optym](../api/x/optym), here we will use `RMSProp` which experientially seems to often be best or top 3 among the optimizers, and is rarely among the worst.  All of the optimizers in prysm have their 'hyperparameters' set when the optimizer is set up.  Hyperparameters are parameters of the optimizer itself, which affect its convergence speed, stability, or other properties.  None of the optimizers in prysm except `F77LBFGSB` are naturally robust, and will either not move meaningfully, or diverge if the hyperparameters are chosen particularly badly.  Generally, you can just change the `alpha` parameter, perhaps starting at a small value like 1e-3, and find the largest $\\alpha$ that doesn't diverge.  \n",
    "\n",
    "The rosenbrock function's minimum is zero at the coordinate `x=1` in all dimensions.  We'll use 2D as an example, because it is easily plottable, but all of the optimizers in prysm work on even extremely high dimensional problems quickly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b3e48-7f9a-4111-be1e-f34d90a3f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from prysm.x.optym import RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab593ccd-260a-4f3b-846e-f2b49d123372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a view of the cost function topology\n",
    "x = np.linspace(-2,2,100)\n",
    "y = np.linspace(-2,2,100)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "z = np.zeros_like(xx)\n",
    "v = np.zeros(2)\n",
    "for i in range(z.shape[0]):\n",
    "    for j in range(z.shape[1]):\n",
    "        v[0] = xx[i,j]\n",
    "        v[1] = yy[i,j]\n",
    "        z[i,j] = rosen(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c1081-1574-4b6e-910b-f9e241d0393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(z, origin='lower', extent=[x.min(), x.max(), y.min(), y.max()], interpolation='lanczos')\n",
    "plt.scatter([1], [1], marker='x', s=30, c='r')\n",
    "plt.text(1.1,1.1,'Minimum', c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babb2d1-b8d3-4880-8b65-ca8b230317ab",
   "metadata": {},
   "source": [
    "To begin optimization, we specify an initial position `x0` and initialize the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adddeac-8161-429b-969a-1b4bd5983fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.asarray([-1,1], dtype=float)\n",
    "opt = RMSProp(cost_grad, x0, alpha=2e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c7713b-8983-41dc-902f-caec3a8b5f9e",
   "metadata": {},
   "source": [
    "With the exception of `F77LBFGSB`, none of the optimizers make any tests of convergence or divergence, and will iterate infinitely if you ask them to.  To iterate, all optimizers simply provide a `def step(self): -> (xk, fk, gk)` method, which returns the parameter vector at the start of that iteration, the cost $\\mathcal{L}$ at the start of that iteration, and the gradient $\\nabla$ at the start of that iteration.  Optimization is as simple as calling step in a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29adf4-04d8-41ea-8b88-205558035578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do 10 iters of optimization\n",
    "for i in range(10):\n",
    "    xk, fk, gk = opt.step()\n",
    "    print(i, fk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e0349-237d-46b9-91d3-b89012e0517e",
   "metadata": {},
   "source": [
    "To improve ergonomics, optym provides a convenience `runN` method which returns a generator yielding (xk, fk, gk).  If you wish to track the progress of optimization in real-time and plot the optimizer trajectory, tqdm integrates well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac16d6-37bc-4701-a8f1-067dd9780982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prysm.x.optym import runN\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbcafc0-567a-4bab-9e7f-13079a87e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize, throwing away progress\n",
    "opt = RMSProp(cost_grad, x0, alpha=2e-2)\n",
    "max_iter = 5_000\n",
    "f_hist = []\n",
    "x_hist = []\n",
    "with tqdm(total=max_iter) as pbar:\n",
    "    for xk, fk, gk in runN(opt, max_iter):\n",
    "        fkf = float(fk)  # if you use cupy as a backend, this will be a size 1 vector\n",
    "        f_hist.append(fkf)\n",
    "        x_hist.append(xk)\n",
    "        pbar.set_description(f'Current cost: {fkf:.3g}', refresh=False)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd85ee-42b9-491c-9c9f-c839155051f8",
   "metadata": {},
   "source": [
    "Now we can plot the cost history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b352d0-8823-438b-85e4-b3a204fa4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(f_hist)\n",
    "plt.gca().set(ylabel='Cost', xlabel='iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a14f7-668d-4055-addb-ea91d2364ad1",
   "metadata": {},
   "source": [
    "We can see that the optimizer stopped making improvement after iteration 1000 or so.  Lets look at the trajectory it followed to see if we can get a clue why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412f85e-b2ec-4362-94b2-f4bb114110c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xh = np.asarray(x_hist)\n",
    "plt.imshow(z, origin='lower', extent=[x.min(), x.max(), y.min(), y.max()], interpolation='lanczos')\n",
    "plt.scatter([1], [1], marker='x', s=30, c='r')\n",
    "plt.plot(*xh.T, c='#FF5555') # transpose to get (x,y) on the front dimension, then unpack because plot wants plot(x, y)\n",
    "plt.text(1.1,1.1,'Minimum', c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43450b3-5068-4d0a-89f2-72e49a255e13",
   "metadata": {},
   "source": [
    "We can see that the optimizer was oscillating from the zig-zag pattern it was making, but was making progress.  Generally oscillation is the result of too much gain, so lets try detuning alpha after 1000 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488605f3-be5d-45cf-9d9e-487e72e9bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy-paste, except for last line\n",
    "opt = RMSProp(cost_grad, x0, alpha=2e-2)\n",
    "max_iter = 5_000\n",
    "f_hist = []\n",
    "x_hist = []\n",
    "with tqdm(total=max_iter) as pbar:\n",
    "    for xk, fk, gk in runN(opt, max_iter):\n",
    "        fkf = float(fk)  # if you use cupy as a backend, this will be a size 1 vector\n",
    "        f_hist.append(fkf)\n",
    "        x_hist.append(xk)\n",
    "        pbar.set_description(f'Current cost: {fkf:.3g}', refresh=False)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if opt.iter == 1000:\n",
    "            opt.alpha /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bdd622-b008-4859-b982-8d1d1995fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(8,4))\n",
    "axs[0].semilogy(f_hist)\n",
    "axs[0].set(ylabel='Cost', xlabel='iteration')\n",
    "\n",
    "xh = np.asarray(x_hist)\n",
    "axs[1].imshow(z, origin='lower', extent=[x.min(), x.max(), y.min(), y.max()], interpolation='lanczos')\n",
    "axs[1].scatter([1], [1], marker='x', s=30, c='r')\n",
    "axs[1].plot(*xh.T, c='#FF5555') # transpose to get (x,y) on the front dimension, then unpack because plot wants plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2092f5-8dec-46be-a87f-c4b2e3e879ea",
   "metadata": {},
   "source": [
    "We can see that detuning the gain took us much closer to the minimum, but we actually could have detuned it much earlier!  And the oscillation is not completely gone.  These all suggest further tuning of RMSProp would lead to better performance on this problem, which is outside the scope of this tutorial.  For those wishing to explore this further, yet another dimension is that most of the optimizers store some sort of internal state.  Most all optimizers have smooth behavior if you change alpha, but you may find the state is somewhat poisoned by the past, and creating a new optimizer with different hyperparameters, initialized at the current best xk does better.  So much to explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd752190-76b5-4c01-ab3f-c0156d928cd5",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "\n",
    "In this tutorial, we learned how to use `x/optym` to perform gradient-based optimization.  We used a difficult to optimize toy problem to exercise the machinery, and began exploring the possibilities afforded to us by adjusting the internal parameters of the optimizer while it is working.  With this information, you should be prepared to create your own cost function and gradient and use these nonlinear optimization techniques to solve problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
