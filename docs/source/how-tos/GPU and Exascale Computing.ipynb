{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU and Exascale Computing\n",
    "\n",
    "prysm is design in the way that it is largely so that it may scale relatively infinitely, and perform computations that are simply beyond the reach of other physical optics programs.  The official model of the Low Order Wavefront Sensor (LOWFS) for the Roman Coronagraph Instrument built with prysm is run at a rate 900,000x higher than its predecesor based on PROPER, for example.\n",
    "\n",
    "In this notebook, we will go through how to achieve such extreme speedups.\n",
    "\n",
    "## GPUs\n",
    "\n",
    "prysm's support for GPUs is implicit; the library uses a shim over numpy and scipy.  If a different library with the same interface but GPUs as an execution unit exist (multiple do) then they can be plugged into prysm and used without issue.  You will likely have the most success using [Cupy](https://cupy.dev/), which is higher quality than Pytorch, Tensorflow, and other ML-intended libraries.  Cupy runs faster with less interface headaches.\n",
    "\n",
    "prysm itself does not import numpy in each file, but performs\n",
    "\n",
    "```python\n",
    "from .mathops import np, fft, ndimage, ...etc\n",
    "```\n",
    "\n",
    "Within mathops, the above-discussed shim is defined, and the `np` variable overwritten.  At startup, `prysm.mathops.np` always wraps numpy, and `ndimage, special, interpolate, fft` wrap scipy.\n",
    "\n",
    "To use a GPU as a backend, simply perform\n",
    "\n",
    "```python\n",
    "import cupy as cp\n",
    "from cupyx.scipy import (\n",
    "    fft as cpfft,\n",
    "    special as cpspecial,\n",
    "    interpolate as cpinterpolate,\n",
    "    ndimage as cpndimage\n",
    ")\n",
    "\n",
    "from prysm.mathops import np, ndimage, interpolate, special, fft\n",
    "\n",
    "np._srcmodule = cp\n",
    "ndimage._srcmodule = cpndimage\n",
    "special._srcmodule = cpspecial\n",
    "interpolate._srcmodule = cpinterpolate\n",
    "fft._srcmodule = cpfft\n",
    "```\n",
    "\n",
    "From this point on, any computation within prysm will be done on the GPU.\n",
    "\n",
    "The majority of GPUs have much better performance with 32-bit floats than with 64-bit floats, so you may also wish to do\n",
    "\n",
    "```python\n",
    "from prysm.conf import config\n",
    "\n",
    "config.precision = 32\n",
    "```\n",
    "\n",
    "This makes prysm use 32 bit numbers for all computations.  Note that if you create your own inputs with 64 bit floats, they will \"infect\" the model due to numpy's promotion rules, and the 32-bit configuration of prysm will be effectively overwritten.\n",
    "\n",
    "Multiple wavelengths can be run in parallel on the same GPU, if the GPU is \"large\" enough for this to make sense, or on multiple GPUs.  The latter requires creating the model's static data on each device (`cp.cuda.runtime.setdevice`) and passing the device number to the map function discussed below.  The results must be collected on either one GPU or on the CPU at the end.\n",
    "\n",
    "## Manycore Systems and CPU parallelization\n",
    "\n",
    "prysm makes no effort at all internally to multi-thread or parallelize computations.  It also does not influence the decisions about this made by its dependencies.  For instance, matrix multiplication is parallelized by numpy when it is linked to intel MKL automatically.\n",
    "\n",
    "To control CPU backends, typically you may set\n",
    "\n",
    "```sh\n",
    "export OPENMP_NUM_THREADS=\"N\"\n",
    "```\n",
    "where N is some integer number you have determined performs well.  Alternatively, use the [threadpoolctl](https://github.com/joblib/threadpoolctl) library to do this for you.  There is also the `fft.set_workers(N)` context manager.\n",
    "\n",
    "As a general comment, the algorithms of physical optics do not \"internally\" parallelize very well.  It is difficult to make a monochromatic problem compute significantly faster on a computer by attempting to parallelize the internal computations.  It is effective to parallelize across wavelengths of a polychromatic problem.  The `concurrent.futures.ThreadPoolExecutor` class and its `map` method are useful here.  `ProcessPoolExecutor` may perform somewhat better.  The pool should be created outside of any loops, as spinning it up and down is a significant cost that need not be paid more than once.\n",
    "\n",
    "These parallelization methods are largely ineffective on consumer desktop platforms due to limited memory bandwidth.  They work better on workstation and server platforms with a larger number of memory channels.  Using > 1 thread per wavelength is often judicious to maximize performance.  This is done by combining the two techniques described just before.\n",
    "\n",
    "## Sense of Capacity\n",
    "\n",
    "As a general sense of how fast prysm can be, multi-plane diffraction models can be run at about 2ms per 7-plane model per wavelength at a total size of 1024x1024 samples, using a Titan XP GPU.  The same model runs in about 50 ms per plane on a dual intel xeon 6248R platform.  The polychromatic model can be made to run at an aggregate time of 60 ms for 9 wavelengths on the GPU and 250 ms for CPU, utilizing all available cores with optimum tuning via threadpoolctl and a ThreadPoolExecutor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
